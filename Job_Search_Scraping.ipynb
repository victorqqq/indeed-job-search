{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Indeed Job Search\n",
    "   \n",
    "   data source: _indeed.com_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The first step is to create a data frame. You only need to do this once, and you can read the excel format later as you proceed.**\n",
    "\n",
    "    Here is an explanation for the five \"results\" columns:\n",
    "        \n",
    "        result_1: both typing and filtering\n",
    "        result_2: only filtering\n",
    "        result_3: only typing\n",
    "        result_4: only typing for job type\n",
    "        result_5: only typing for location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=('date', 'keyword', 'job type','location','result_1','result_2','result_3','result_4','result_5'))\n",
    "\n",
    "df = pd.to_excel(r\"C:\\Users\\qiuwk\\Google Drive\\Projects\\Job_Search.xlsx\") # Make adjustment to the path when necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then, this is the code I run everyday for daily collecton of data.**\n",
    "\n",
    "**_NOTE: Pay Attention to variable \"a\"_** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date\n",
    "\n",
    "df = pd.read_excel(r\"C:\\Users\\qiuwk\\Google Drive\\Projects\\Job_Search.xlsx\", index_col = 0) # Make adjustment to the path when necessary.\n",
    "\n",
    "date = date.today()\n",
    "base_url = 'https://www.indeed.com/jobs?'\n",
    "query = ['data', 'software+engineer','web+develop','finance','audit','sales']\n",
    "job_type = ['fulltime','parttime','contract']\n",
    "cities = ['Los+Angeles','San+Francisco','Seattle','Boston','New+York','Houston','Chicago']\n",
    "states = ['CA','CA', 'WA','MA','NY','TX','IL']\n",
    "\n",
    "a = 1 # You need to change it everyday because it is the index of the row you will start today\n",
    "for x in cities:\n",
    "    for y in job_type:\n",
    "        for z in query:\n",
    "            i = cities.index(x)\n",
    "            \n",
    "            # using both typing and filtering\n",
    "            url_one = base_url + 'q=' + y + '+' + z + '+'+ x + '&l=' + x + ',+' + states[i] + '&jt='+ y\n",
    "            # using only filtering\n",
    "            url_two = base_url + 'q=' + z + '&l=' + x + ',+' + states[i] + '&jt='+ y\n",
    "            # using only typing\n",
    "            url_three = base_url + 'q=' + x + '+' + y + '+' + z\n",
    "            # using only typing for job type\n",
    "            url_four = base_url + 'q=' + y + '+' + z + '&l='+ x + ',+' + states[i] \n",
    "            # using only typing for location\n",
    "            url_five = base_url + 'q=' + x + '+' + z + '&jt='+ y\n",
    "            \n",
    "            data = [a,date,z, y, x]\n",
    "            \n",
    "            for j in range(5):\n",
    "                # Determine which url to use\n",
    "                if j == 0:\n",
    "                    html = urlopen(url_one).read()\n",
    "                elif j == 1:\n",
    "                    html = urlopen(url_two).read()\n",
    "                elif j == 2:\n",
    "                    html = urlopen(url_three).read()\n",
    "                elif j == 3:\n",
    "                    html = urlopen(url_four).read()\n",
    "                elif j ==4:\n",
    "                    html = urlopen(url_five).read()\n",
    "                #Read the url and get the result\n",
    "                soup = BeautifulSoup(html,features = 'lxml')        \n",
    "                text = soup.find(id = 'searchCountPages')\n",
    "                if text is None:\n",
    "                    result = 0\n",
    "                else:\n",
    "                    result_list = text.get_text().split()\n",
    "                    result = result_list[-2]\n",
    "                #Add result to the data\n",
    "                data.append(result)\n",
    "            \n",
    "            df.loc[a] =data \n",
    "            print(df.loc[a])\n",
    "            a += 1 \n",
    "\n",
    "print(a) # This is the \"a\" you want to use in the next collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before saving the data, I would like to take a look of it. But, you can just save it as well.**\n",
    "\n",
    "_So, if you want to check the data, run the \"df\" argument. Otherwise, skip it and save the data to your spreadsheet._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(r\"C:\\Users\\qiuwk\\Google Drive\\Projects\\Job_Search.xlsx\") # Make adjustment to the path when necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion & Mistake\n",
    "    This is the web scraping process of my project. I will utilize this dataset to perform analysis on what is the best to search a job on Indeed.com. \n",
    "    For improvements, I defintely want to get the whole process more automatic. Since I still need to manually change the variable \"a\" for every day, it becomes somehow not very effcient. Also, I think my loop can be shorten, but I have not come up a better idea yet. \n",
    "    Also, I was trying to web-scrap other websites like LinkedIn, Dice, and ZipRecruiter; however, due to my limitation of web scraping skills, I did not finish that part. I hope someone can give me some hints on that and I can perform a comparative analysis among these popular job-search websites."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
